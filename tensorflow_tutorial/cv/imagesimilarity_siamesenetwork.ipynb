{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image similarity using Siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install VGGFACE library whose pretrained model is used except a last few layers, which are finetuned per use case.\n",
    "# ! pip install git+https://github.com/rcmalli/keras-vggface.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from keras_vggface.utils import preprocess_input\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing import image\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pandas' from '/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\r\n",
      "\u001b[33mWARNING: Package(s) not found: cvxopt\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip show cvxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data location\n",
    "parent_dir = '/home/ec2-user/ebs/data/cv_data/recognizing-faces-in-the-wild'\n",
    "trainfile_path = os.path.join(parent_dir, 'train_relationships.csv')\n",
    "trainfolder_path = os.path.join(parent_dir, 'train/')\n",
    "testfolder_path = os.path.join(parent_dir, 'test')\n",
    "val_families = 'F09'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ec2-user/ebs/data/cv_data/recognizing-faces-in-the-wild/train/F0238/MID4/P02522_face2.jpg',\n",
       " '/home/ec2-user/ebs/data/cv_data/recognizing-faces-in-the-wild/train/F0238/MID4/P02524_face4.jpg',\n",
       " '/home/ec2-user/ebs/data/cv_data/recognizing-faces-in-the-wild/train/F0238/MID4/P02523_face2.jpg',\n",
       " '/home/ec2-user/ebs/data/cv_data/recognizing-faces-in-the-wild/train/F0238/MID3/P02521_face2.jpg',\n",
       " '/home/ec2-user/ebs/data/cv_data/recognizing-faces-in-the-wild/train/F0238/MID5/P02524_face2.jpg']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images = glob.glob(trainfolder_path + '*/*/*.jpg')\n",
    "all_images[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#(training images) 11232\n",
      "#(validation images) 1147\n"
     ]
    }
   ],
   "source": [
    "train_images = [x for x in all_images if val_families not in x]\n",
    "val_images = [x for x in all_images if val_families in x]\n",
    "print('#(training images)', len(train_images))\n",
    "print('#(validation images)', len(val_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F0238/MID4', 'F0238/MID4', 'F0238/MID4', 'F0238/MID3', 'F0238/MID5', 'F0238/MID6', 'F0238/MID1', 'F0238/MID1', 'F0238/MID1', 'F0238/MID1']\n"
     ]
    }
   ],
   "source": [
    "ppl = [x.split('/')[-3] + '/' + x.split('/')[-2] for x in all_images]\n",
    "print(ppl[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "train_person_to_images_map = defaultdict(list)\n",
    "for sample_img in train_images:\n",
    "    train_person_to_images_map[sample_img.split('/')[-3] + '/' + sample_img.split('/')[-2]].append(sample_img)\n",
    "    \n",
    "val_person_to_images_map = defaultdict(list)\n",
    "for sample_img in train_images:\n",
    "    val_person_to_images_map[sample_img.split('/')[-3] + '/' + sample_img.split('/')[-2]].append(sample_img)\n",
    "    \n",
    "train_person_to_images_map = dict(train_person_to_images_map)\n",
    "val_person_to_images_map = dict(val_person_to_images_map)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA\n",
    "for k in train_person_to_images_map.keys():\n",
    "    if len(train_person_to_images_map[k]) == 0:\n",
    "        print(\"Train Damn\")\n",
    "        print(k)\n",
    "        \n",
    "\n",
    "for k in val_person_to_images_map.keys():\n",
    "    if len(val_person_to_images_map[k]) == 0:\n",
    "        print(\"VAL Damn\")\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of relationnship pairs: 3598\n",
      "After filtering, total number of relationnship pairs: 3362\n"
     ]
    }
   ],
   "source": [
    "relationships = pd.read_csv(trainfile_path)\n",
    "relationships = list(zip(relationships['p1'].values, relationships['p2'].values))\n",
    "print('Total number of relationnship pairs:', len(relationships))\n",
    "relationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]\n",
    "print('After filtering, total number of relationnship pairs:', len(relationships))                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect train and validation set of relationship pairs\n",
    "train = [x for x in relationships if val_families not in x[0]]\n",
    "val = [x for x in relationships if val_families in x[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('F0002/MID1', 'F0002/MID3'), ('F0002/MID2', 'F0002/MID3'), ('F0005/MID1', 'F0005/MID2'), ('F0005/MID3', 'F0005/MID2'), ('F0009/MID1', 'F0009/MID4')]\n",
      "Total training pairs 3066\n"
     ]
    }
   ],
   "source": [
    "# QA\n",
    "print(train[:5])\n",
    "print('Total training pairs', len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('F0900/MID2', 'F0900/MID1'), ('F0900/MID3', 'F0900/MID1'), ('F0901/MID1', 'F0901/MID4'), ('F0901/MID2', 'F0901/MID1'), ('F0901/MID2', 'F0901/MID4')]\n",
      "Total validation pairs 296\n"
     ]
    }
   ],
   "source": [
    "print(val[:5])\n",
    "print('Total validation pairs', len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(path):\n",
    "    img = image.load_img(path, target_size=(197, 197))\n",
    "    img = np.array(img).astype('float')\n",
    "    return preprocess_input(img, version=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice, sample\n",
    "\n",
    "def gen(list_tuples, person_to_images_map, batch_size=16):\n",
    "    ppl = list(person_to_images_map.keys())\n",
    "    while True:\n",
    "        # sample(): chooses k unique random elements from a population sequence\n",
    "        # filling half batch tuples with 1 labels \n",
    "        batch_tuples = sample(list_tuples, batch_size // 2)\n",
    "        # assign 1 label to all pairs given in relationships file\n",
    "        labels = [1] * len(batch_tuples)\n",
    "        # filling half batch tuples with 0 labels \n",
    "        while len(batch_tuples) < batch_size:\n",
    "            p1 = choice(ppl)\n",
    "            p2 = choice(ppl)\n",
    "            \n",
    "            if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n",
    "                batch_tuples.append((p1, p2))\n",
    "                labels.append(0)\n",
    "        \n",
    "        for x in batch_tuples:\n",
    "            if not len(person_to_images_map[x[0]]):\n",
    "                print(x[0])\n",
    "            \n",
    "        # select a single image out of many provided \n",
    "        X0 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n",
    "        X0 = np.array([read_img(x) for x in X0])\n",
    "        \n",
    "        X1 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n",
    "        X1 = np.array([read_img(x) for x in X1])\n",
    "        \n",
    "    return [X0, X1], labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Flatten, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Add, Conv2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras_vggface.vggface import VGGFace\n",
    "\n",
    "def baseline_model():\n",
    "    input_1 = Input(shape=(197, 197, 3))\n",
    "    input_2 = Input(shape=(197, 197, 3))\n",
    "    \n",
    "    base_model = VGGFace(model='resnet50', include_top=False)\n",
    "    \n",
    "    # use pretrained mdoel for all layers except last three layers\n",
    "    # top layers used for feature engineering\n",
    "    # later layers are fine tuned to make decisions\n",
    "    for x in base_model.layers[:-3]:\n",
    "        x.trainiable = False\n",
    "          \n",
    "    x1 = base_model(input_1)\n",
    "    x2 = base_model(input_2)\n",
    "    \n",
    "    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n",
    "    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n",
    "    \n",
    "    x3 = Subtract()([x1, x2])\n",
    "    x3 = Multiply()([x3, x3])\n",
    "    \n",
    "    x1_ = Multiply()([x1, x1])\n",
    "    x2_ = Multiply()([x2, x2])\n",
    "    x4 = Subtract()([x1_, x2_])\n",
    "    x = Concatenate(axis=-1)([x4, x3])\n",
    "    \n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dropout(0.01)(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model([input_1, input_2], out)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', metrics = ['acc', auroc], optimizer = Adam(0.0001))\n",
    "    \n",
    "    model.summary()\n",
    "                  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Downloading data from https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_notop_resnet50.h5\n",
      "94699520/94694792 [==============================] - 1s 0us/step\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From <ipython-input-15-0345cd89dcf8>:2: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 197, 197, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 197, 197, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_resnet50 (Model)        multiple             23561152    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalM (None, 2048)         0           vggface_resnet50[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           vggface_resnet50[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_2 (GlobalM (None, 2048)         0           vggface_resnet50[2][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 2048)         0           vggface_resnet50[2][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4096)         0           global_max_pooling2d_1[0][0]     \n",
      "                                                                 global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 4096)         0           global_max_pooling2d_2[0][0]     \n",
      "                                                                 global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 4096)         0           concatenate_1[0][0]              \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 4096)         0           concatenate_2[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 4096)         0           concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "subtract_2 (Subtract)           (None, 4096)         0           multiply_2[0][0]                 \n",
      "                                                                 multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 4096)         0           subtract_1[0][0]                 \n",
      "                                                                 subtract_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 8192)         0           subtract_2[0][0]                 \n",
      "                                                                 multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          819300      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            101         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 24,380,553\n",
      "Trainable params: 24,327,433\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "file_path = 'vgg_face.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "reduce_on_plateau = ReduceLROnPlateau(monitor='val_acc', mode='max', factor=0.1, patience=20, verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint, reduce_on_plateau]\n",
    "\n",
    "model = baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(gen(train, train_person_to_images_map, batch_size=16), \n",
    "                    use_multiprocessing=True, \n",
    "                    validation_data=gen(val, val_person_to_images_map, batch_size=16), \n",
    "                    epochs=10, verbose=1, workers=4, callbacks=callbacks_list, \n",
    "                    steps_per_epoch=200, validation_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(seq, size=32):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "submission = pd.read_csv(os.path.join(parent_path, 'sample_submission.csv'))\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for batch in tqdm(chunker(submission.img_pair.values)):\n",
    "    X1 = [x.split(\"-\")[0] for x in batch]\n",
    "    X1 = np.array([read_img(test_path + x) for x in X1])\n",
    "\n",
    "    X2 = [x.split(\"-\")[1] for x in batch]\n",
    "    X2 = np.array([read_img(test_path + x) for x in X2])\n",
    "\n",
    "    pred = model.predict([X1, X2]).ravel().tolist()\n",
    "    predictions += pred\n",
    "\n",
    "submission['is_related'] = predictions\n",
    "\n",
    "submission.to_csv(os.path.join(parent_path, \"vgg_face.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
